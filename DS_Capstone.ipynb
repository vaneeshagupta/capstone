{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "Dimensional data mart that represents a simple business involving customers, the products, and the vendors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.5/libexec\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Global Variables and Paths for Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"adventureworks\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"Ashwaniis#1!\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"local\", # \"atlas\"\n",
    "    \"user_name\" : \"vaneeshagupta10\",\n",
    "    \"password\" : \"Fdztq26kWFlyBXiE\",\n",
    "    \"cluster_name\" : \"cluster0\",\n",
    "    \"cluster_subnet\" : \"koqso\",\n",
    "    \"db_name\" : \"northwind\",\n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = \"/Users/vaneeshagupta/Desktop/capstone\"\n",
    "data_dir = base_dir\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Streaming Data Source Directories\n",
    "# --------------------------------------------------------------------------------\n",
    "orders_stream_dir = os.path.join(stream_dir, 'orders')\n",
    "purchase_orders_stream_dir = os.path.join(stream_dir, 'purchase_orders')\n",
    "inventory_trans_stream_dir = os.path.join(stream_dir, 'inventory_transactions')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Databricks/SQL Warehouse Paths (Bronze, Silver, Gold Layers)\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"data_mart_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "orders_output_bronze = os.path.join(database_dir, 'fact_orders', 'bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'fact_orders', 'silver')\n",
    "orders_output_gold = os.path.join(database_dir, 'fact_orders', 'gold')\n",
    "\n",
    "purchase_orders_output_bronze = os.path.join(database_dir, 'fact_purchase_orders', 'bronze')\n",
    "purchase_orders_output_silver = os.path.join(database_dir, 'fact_purchase_orders', 'silver')\n",
    "purchase_orders_output_gold = os.path.join(database_dir, 'fact_purchase_orders', 'gold')\n",
    "\n",
    "inventory_trans_output_bronze = os.path.join(database_dir, 'fact_inventory_transactions', 'bronze')\n",
    "inventory_trans_output_silver = os.path.join(database_dir, 'fact_inventory_transactions', 'silver')\n",
    "inventory_trans_output_gold = os.path.join(database_dir, 'fact_inventory_transactions', 'gold')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Global Variables for MySQL, MongoDB, and File Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batches\")\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    return df_dropped\n",
    "\n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"driver\", args['conn_props']['driver']) \\\n",
    "        .option(\"user\", args['conn_props']['user']) \\\n",
    "        .option(\"password\", args['conn_props']['password']) \\\n",
    "        .option(\"query\", sql_query) \\\n",
    "        .load()\n",
    "    return dframe\n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "    return uri\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Northwind Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "    return client\n",
    "    \n",
    "def set_mongo_collections_with_pyspark(spark_session, data_directory: str, json_files: dict, **mongo_args):\n",
    "    db_name = mongo_args[\"db_name\"]\n",
    "    mongo_uri = get_mongo_uri(**mongo_args)\n",
    "\n",
    "    for collection_name, filename in json_files.items():\n",
    "        json_file_path = os.path.join(data_directory, filename)\n",
    "\n",
    "        df = spark_session.read \\\n",
    "            .option(\"multiline\", \"true\") \\\n",
    "            .json(json_file_path)\n",
    "\n",
    "        df.write \\\n",
    "            .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"uri\", mongo_uri) \\\n",
    "            .option(\"database\", db_name) \\\n",
    "            .option(\"collection\", collection_name) \\\n",
    "            .save()\n",
    "\n",
    "        print(f\"✔ Loaded {filename} into MongoDB collection '{collection_name}'\")\n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "    dframe = dframe.drop('_id')\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    return dframe\n",
    "\n",
    "\n",
    "def get_sql_dataframe(sql_query, **args):\n",
    "    '''Create a connection to the MySQL database and return a DataFrame'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    \n",
    "    # Using context manager (with statement) to automatically manage connection\n",
    "    with sqlEngine.connect() as connection:\n",
    "        # Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame\n",
    "        dframe = pd.read_sql(text(sql_query), connection)\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def set_dataframe(df, table_name, pk_column, db_operation, **args):\n",
    "    '''Create a connection to the MySQL database and insert/update data'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    \n",
    "    # Using context manager (with statement) to automatically manage connection\n",
    "    with sqlEngine.connect() as connection:\n",
    "        '''Invoke the Pandas DataFrame .to_sql() function to either create, or append to, a table'''\n",
    "        if db_operation == \"insert\":\n",
    "            df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "            connection.execute(text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\"))\n",
    "        elif db_operation == \"update\":\n",
    "            df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory '/Users/vaneeshagupta/Desktop/capstone/spark-warehouse/data_mart_dlh.db' has been removed successfully.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/3.5.5/libexec/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/vaneeshagupta/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/vaneeshagupta/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6553545a-6bc3-46e7-af51-c98532a81438;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 122ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6553545a-6bc3-46e7-af51-c98532a81438\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "25/05/09 16:58:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://vaneeshas-air:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Northwind Data Lakehouse (Medallion Architecture)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x12e545bb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "#mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.3.0.jar\") \n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a New Metadata Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Lab 06 Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Lab 6.0');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Reference Data from MongoDB, MySQL, and CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MongoDB (Note: Customer Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Loaded customer.json into MongoDB collection 'customers'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>ModifiedDate</th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>rowguid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AW00000001</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AW00000002</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AccountNumber  CustomerID CustomerType         ModifiedDate  TerritoryID  \\\n",
       "0    AW00000001           1            S  2004-10-13 11:15:07            1   \n",
       "1    AW00000002           2            S  2004-10-13 11:15:07            1   \n",
       "\n",
       "  rowguid  \n",
       "0          \n",
       "1          "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JSON files for MongoDB collections\n",
    "json_files = {\n",
    "    \"customers\": \"customer.json\"\n",
    "}\n",
    "\n",
    "# Set MongoDB collections with the specified JSON files\n",
    "set_mongo_collections_with_pyspark(spark, data_dir, json_files, **mongodb_args)\n",
    "\n",
    "\n",
    "mongodb_args[\"collection\"] = \"customers\"\n",
    "\n",
    "df_dim_customers = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_customers.toPandas().head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AccountNumber: string (nullable = true)\n",
      " |-- CustomerID: long (nullable = true)\n",
      " |-- CustomerType: string (nullable = true)\n",
      " |-- ModifiedDate: string (nullable = true)\n",
      " |-- TerritoryID: long (nullable = true)\n",
      " |-- rowguid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_customers.printSchema()\n",
    "\n",
    "\n",
    "# Rename columns to standardize them as per dimension schema\n",
    "df_dim_customers = df_dim_customers.withColumnRenamed(\"CustomerID\", \"customer_id\") \\\n",
    "                                    .withColumnRenamed(\"AccountNumber\", \"account_number\") \\\n",
    "                                    .withColumnRenamed(\"CustomerType\", \"customer_type\") \\\n",
    "                                    .withColumnRenamed(\"ModifiedDate\", \"modified_date\") \\\n",
    "                                    .withColumnRenamed(\"TerritoryID\", \"territory_id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        customer_key|                 int|   NULL|\n",
      "|         customer_id|              bigint|   NULL|\n",
      "|      account_number|              string|   NULL|\n",
      "|       customer_type|              string|   NULL|\n",
      "|       modified_date|              string|   NULL|\n",
      "|        territory_id|              bigint|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|       data_mart_dlh|       |\n",
      "|               Table|       dim_customers|       |\n",
      "|        Created Time|Fri May 09 16:58:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/vanee...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>account_number</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>modified_date</th>\n",
       "      <th>territory_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AW00000001</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>AW00000002</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id account_number customer_type  \\\n",
       "0             1            1     AW00000001             S   \n",
       "1             2            2     AW00000002             S   \n",
       "\n",
       "         modified_date  territory_id  \n",
       "0  2004-10-13 11:15:07             1  \n",
       "1  2004-10-13 11:15:07             1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create or replace a temporary view for 'dim_customers'\n",
    "df_dim_customers.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "# SQL query to add ROW_NUMBER as the primary key (customer_key)\n",
    "sql_customers = \"\"\"\n",
    "SELECT \n",
    "    ROW_NUMBER() OVER (ORDER BY customer_id) AS customer_key,\n",
    "    customer_id,\n",
    "    account_number,\n",
    "    customer_type,\n",
    "    modified_date,\n",
    "    territory_id\n",
    "FROM customers\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Execute SQL query to get the updated DataFrame with the primary key\n",
    "df_dim_customers = spark.sql(sql_customers)\n",
    "\n",
    "# Reorder Columns\n",
    "ordered_columns = ['customer_key', 'customer_id', 'account_number', 'customer_type', \n",
    "                   'modified_date', 'territory_id']\n",
    "\n",
    "\n",
    "df_dim_customers = df_dim_customers.select(*ordered_columns)\n",
    "\n",
    "# Unit test: Show first two rows\n",
    "df_dim_customers.toPandas().head(2)\n",
    "\n",
    "# Save the DataFrame as a table in MySQL (data mart)\n",
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")\n",
    "\n",
    "# Unit test: Describe the table\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_customers\").show()\n",
    "\n",
    "# Preview the first 2 rows in the table\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MySQL (Note: Product Data and Date Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- ProductNumber: string (nullable = true)\n",
      " |-- MakeFlag: boolean (nullable = true)\n",
      " |-- FinishedGoodsFlag: boolean (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- SafetyStockLevel: integer (nullable = true)\n",
      " |-- ReorderPoint: integer (nullable = true)\n",
      " |-- StandardCost: double (nullable = true)\n",
      " |-- ListPrice: double (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- SizeUnitMeasureCode: string (nullable = true)\n",
      " |-- WeightUnitMeasureCode: string (nullable = true)\n",
      " |-- Weight: decimal(8,2) (nullable = true)\n",
      " |-- DaysToManufacture: integer (nullable = true)\n",
      " |-- ProductLine: string (nullable = true)\n",
      " |-- Class: string (nullable = true)\n",
      " |-- Style: string (nullable = true)\n",
      " |-- ProductSubcategoryID: integer (nullable = true)\n",
      " |-- ProductModelID: integer (nullable = true)\n",
      " |-- SellStartDate: timestamp (nullable = true)\n",
      " |-- SellEndDate: timestamp (nullable = true)\n",
      " |-- DiscontinuedDate: timestamp (nullable = true)\n",
      " |-- rowguid: binary (nullable = true)\n",
      " |-- ModifiedDate: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define SQL query to fetch all product data from the MySQL 'product' table\n",
    "sql_dim_products = f\"SELECT * FROM {mysql_args['db_name']}.product\"\n",
    "\n",
    "# Execute the query and load the result into a Spark DataFrame\n",
    "df_dim_products = get_mysql_dataframe(spark, sql_dim_products, **mysql_args)\n",
    "\n",
    "# Display the schema of the DataFrame for verification\n",
    "df_dim_products.printSchema()\n",
    "# Preview the first two rows to validate data\n",
    "df_dim_products.toPandas().head(2)\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Define window specification for row numbering based on ProductID\n",
    "window_spec = Window.orderBy(\"ProductID\")\n",
    "\n",
    "# Add a new surrogate key column 'product_key' using ROW_NUMBER()\n",
    "df_dim_products = df_dim_products.withColumn(\"product_key\", row_number().over(window_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>ListPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  ProductID             Name ProductNumber  ListPrice\n",
       "0            1          1  Adjustable Race       AR-5381        0.0\n",
       "1            2          2     Bearing Ball       BA-8327        0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Use ProductID as the natural sort column\n",
    "window_spec = Window.orderBy(\"ProductID\")\n",
    "\n",
    "# Add surrogate primary key\n",
    "df_dim_products = df_dim_products.withColumn(\"product_key\", row_number().over(window_spec))\n",
    "\n",
    "ordered_columns = [\"product_key\", \"ProductID\", \"Name\", \"ProductNumber\", \"ListPrice\"]\n",
    "\n",
    "# Reorder\n",
    "df_dim_products = df_dim_products.select(*ordered_columns)\n",
    "\n",
    "# Show top 2 rows\n",
    "df_dim_products.toPandas().head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed DataFrame as a table in the local Spark SQL warehouse\n",
    "df_dim_products.write.saveAsTable(f\"{dest_database}.dim_products\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         product_key|                 int|   NULL|\n",
      "|           ProductID|                 int|   NULL|\n",
      "|                Name|         varchar(50)|   NULL|\n",
      "|       ProductNumber|         varchar(25)|   NULL|\n",
      "|           ListPrice|              double|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|       data_mart_dlh|       |\n",
      "|               Table|        dim_products|       |\n",
      "|        Created Time|Fri May 09 16:58:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/vanee...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>ListPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  ProductID             Name ProductNumber  ListPrice\n",
       "0            1          1  Adjustable Race       AR-5381        0.0\n",
       "1            2          2     Bearing Ball       BA-8327        0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe to verify\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_products\").show()\n",
    "\n",
    "# Preview rows\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_products LIMIT 2\").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the Date Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = f\"SELECT * FROM {mysql_args['db_name']}.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)\n",
    "\n",
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            date_key|      int|   NULL|\n",
      "|           full_date|     date|   NULL|\n",
      "|           date_name| char(11)|   NULL|\n",
      "|        date_name_us| char(11)|   NULL|\n",
      "|        date_name_eu| char(11)|   NULL|\n",
      "|         day_of_week|  tinyint|   NULL|\n",
      "|    day_name_of_week| char(10)|   NULL|\n",
      "|        day_of_month|  tinyint|   NULL|\n",
      "|         day_of_year|      int|   NULL|\n",
      "|     weekday_weekend| char(10)|   NULL|\n",
      "|        week_of_year|  tinyint|   NULL|\n",
      "|          month_name| char(10)|   NULL|\n",
      "|       month_of_year|  tinyint|   NULL|\n",
      "|is_last_day_of_month|  char(1)|   NULL|\n",
      "|    calendar_quarter|  tinyint|   NULL|\n",
      "|       calendar_year|      int|   NULL|\n",
      "| calendar_year_month| char(10)|   NULL|\n",
      "|   calendar_year_qtr| char(10)|   NULL|\n",
      "|fiscal_month_of_year|  tinyint|   NULL|\n",
      "|      fiscal_quarter|  tinyint|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000/01/01</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2000/01/02</td>\n",
       "      <td>01/02/2000</td>\n",
       "      <td>02/01/2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date    date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0  20000101  2000-01-01  2000/01/01   01/01/2000   01/01/2000             7   \n",
       "1  20000102  2000-01-02  2000/01/02   01/02/2000   02/01/2000             1   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0       Saturday               1            1      Weekend     ...   \n",
       "1       Sunday                 2            2      Weekend     ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2000          2000-01      \n",
       "1                     N                1           2000          2000-01      \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0         2000Q1                         7              3        2000   \n",
       "1         2000Q1                         7              3        2000   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0         2000-07          2000Q3      \n",
       "1         2000-07          2000Q3      \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV File Using PySpark (Note: Vendor Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vaneeshagupta/Desktop/capstone/vendor.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>Name</th>\n",
       "      <th>CreditRating</th>\n",
       "      <th>PreferredVendorStatus</th>\n",
       "      <th>ActiveFlag</th>\n",
       "      <th>PurchasingWebServiceURL</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>INTERNAT0001</td>\n",
       "      <td>International</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>2002-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ELECTRON0002</td>\n",
       "      <td>Electronic Bike Repair &amp; Supplies</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>2002-02-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID AccountNumber                               Name  CreditRating  \\\n",
       "0         1  INTERNAT0001                      International             1   \n",
       "1         2  ELECTRON0002  Electronic Bike Repair & Supplies             1   \n",
       "\n",
       "   PreferredVendorStatus  ActiveFlag PurchasingWebServiceURL ModifiedDate  \n",
       "0                      1           1                    NULL   2002-02-25  \n",
       "1                      1           1                    NULL   2002-02-17  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1). Get a reference to the 'vendor.csv' file located in your batch directory.\n",
    "vendor_csv = \"/Users/vaneeshagupta/Desktop/capstone/vendor.csv\"\n",
    "print(vendor_csv)\n",
    "\n",
    "# 2). Use Spark to read the CSV file into the 'df_dim_vendor' DataFrame.\n",
    "#     Set header=True to use the first row as column names, and infer the schema automatically.\n",
    "df_dim_vendor = spark.read.format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .load(vendor_csv)\n",
    "\n",
    "# 3). Unit Test: Convert to Pandas and show the first two rows for verification.\n",
    "df_dim_vendor.toPandas().head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_key</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>vendor_name</th>\n",
       "      <th>credit_rating</th>\n",
       "      <th>active_flag</th>\n",
       "      <th>modified_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Electronic Bike Repair &amp; Supplies</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-02-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendor_key  vendor_id                        vendor_name  credit_rating  \\\n",
       "0           1          1                      International              1   \n",
       "1           2          2  Electronic Bike Repair & Supplies              1   \n",
       "\n",
       "   active_flag modified_date  \n",
       "0            1    2002-02-25  \n",
       "1            1    2002-02-17  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename columns to follow consistent naming conventions (snake_case)\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_vendor = df_dim_vendor \\\n",
    "    .withColumnRenamed(\"VendorID\", \"vendor_id\") \\\n",
    "    .withColumnRenamed(\"Name\", \"vendor_name\") \\\n",
    "    .withColumnRenamed(\"CreditRating\", \"credit_rating\") \\\n",
    "    .withColumnRenamed(\"ActiveFlag\", \"active_flag\") \\\n",
    "    .withColumnRenamed(\"ModifiedDate\", \"modified_date\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER()\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_vendor.createOrReplaceTempView(\"vendor\")\n",
    "\n",
    "sql_vendor = \"\"\"\n",
    "    SELECT \n",
    "        ROW_NUMBER() OVER (ORDER BY vendor_id) AS vendor_key,\n",
    "        vendor_id,\n",
    "        vendor_name,\n",
    "        credit_rating,\n",
    "        active_flag,\n",
    "        modified_date\n",
    "    FROM vendor\n",
    "\"\"\"\n",
    "\n",
    "df_dim_vendor = spark.sql(sql_vendor)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = [\n",
    "    'vendor_key', 'vendor_id', 'vendor_name',\n",
    "    'credit_rating', 'active_flag', 'modified_date'\n",
    "]\n",
    "\n",
    "df_dim_vendor = df_dim_vendor.select(*ordered_columns)\n",
    "df_dim_vendor.toPandas().head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Save the dimension table to the data mart (local Spark SQL warehouse)\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_vendor.write.saveAsTable(f\"{dest_database}.dim_vendor\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|          vendor_key|                 int|   NULL|\n",
      "|           vendor_id|                 int|   NULL|\n",
      "|         vendor_name|              string|   NULL|\n",
      "|       credit_rating|                 int|   NULL|\n",
      "|         active_flag|                 int|   NULL|\n",
      "|       modified_date|           timestamp|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|       data_mart_dlh|       |\n",
      "|               Table|          dim_vendor|       |\n",
      "|        Created Time|Fri May 09 16:58:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/vanee...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_key</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>vendor_name</th>\n",
       "      <th>credit_rating</th>\n",
       "      <th>active_flag</th>\n",
       "      <th>modified_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Electronic Bike Repair &amp; Supplies</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2002-02-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendor_key  vendor_id                        vendor_name  credit_rating  \\\n",
       "0           1          1                      International              1   \n",
       "1           2          2  Electronic Bike Repair & Supplies              1   \n",
       "\n",
       "   active_flag modified_date  \n",
       "0            1    2002-02-25  \n",
       "1            1    2002-02-17  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Unit Test: Describe and Preview Table\n",
    "# ----------------------------------------------------------------------------------\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_vendor\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_vendor LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+--------------+----------+---------+---------+----------+----------+\n",
      "|sales_order_id|customer_id|order_date|order_date_key|product_id|vendor_id|order_qty|line_total|total_due |\n",
      "+--------------+-----------+----------+--------------+----------+---------+---------+----------+----------+\n",
      "|43659         |676        |2001-06-30|20010630      |773       |0        |2        |4079.988  |27231.5495|\n",
      "|43671         |200        |2001-06-30|20010630      |732       |0        |2        |713.796   |10784.9873|\n",
      "|43674         |83         |2001-06-30|20010630      |758       |0        |3        |2624.382  |3479.9306 |\n",
      "|43687         |269        |2001-06-30|20010630      |768       |0        |1        |419.4589  |1668.6076 |\n",
      "|43688         |161        |2001-06-30|20010630      |758       |0        |2        |1749.588  |16891.0829|\n",
      "|43692         |221        |2001-06-30|20010630      |732       |0        |3        |1070.694  |51056.4965|\n",
      "|43697         |21768      |2001-06-30|20010630      |749       |0        |1        |3578.27   |3953.9884 |\n",
      "|43708         |20042      |2001-07-02|20010702      |764       |0        |1        |699.0982  |772.5036  |\n",
      "|43711         |27606      |2001-07-03|20010703      |753       |0        |1        |3578.27   |3953.9884 |\n",
      "|43712         |13513      |2001-07-03|20010703      |750       |0        |1        |3578.27   |3953.9884 |\n",
      "+--------------+-----------+----------+--------------+----------+---------+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Load source fact data\n",
    "df_sales_order_header = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:mysql://{mysql_args['host_name']}:{mysql_args['port']}/{mysql_args['db_name']}\") \\\n",
    "    .option(\"driver\", mysql_args['conn_props']['driver']) \\\n",
    "    .option(\"dbtable\", \"SalesOrderHeader\") \\\n",
    "    .option(\"user\", mysql_args['conn_props']['user']) \\\n",
    "    .option(\"password\", mysql_args['conn_props']['password']) \\\n",
    "    .load() \\\n",
    "    .select(\"SalesOrderID\", \"CustomerID\", \"OrderDate\", \"TotalDue\")\n",
    "\n",
    "df_sales_order_detail = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:mysql://{mysql_args['host_name']}:{mysql_args['port']}/{mysql_args['db_name']}\") \\\n",
    "    .option(\"driver\", mysql_args['conn_props']['driver']) \\\n",
    "    .option(\"dbtable\", \"SalesOrderDetail\") \\\n",
    "    .option(\"user\", mysql_args['conn_props']['user']) \\\n",
    "    .option(\"password\", mysql_args['conn_props']['password']) \\\n",
    "    .load() \\\n",
    "    .select(\"SalesOrderID\", \"ProductID\", \"OrderQty\", \"LineTotal\")\n",
    "\n",
    "# Join header and detail tables\n",
    "df_fact_orders = df_sales_order_header.join(df_sales_order_detail, on=\"SalesOrderID\", how=\"left\")\n",
    "\n",
    "# Join with product-vendor bridge\n",
    "df_product_vendor = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:mysql://{mysql_args['host_name']}:{mysql_args['port']}/{mysql_args['db_name']}\") \\\n",
    "    .option(\"driver\", mysql_args['conn_props']['driver']) \\\n",
    "    .option(\"dbtable\", \"productvendor\") \\\n",
    "    .option(\"user\", mysql_args['conn_props']['user']) \\\n",
    "    .option(\"password\", mysql_args['conn_props']['password']) \\\n",
    "    .load() \\\n",
    "    .select(\"ProductID\", \"VendorID\")\n",
    "\n",
    "df_fact_orders = df_fact_orders.join(df_product_vendor, on=\"ProductID\", how=\"left\")\n",
    "\n",
    "# Load dimension tables\n",
    "df_dim_customers = spark.table(\"data_mart_dlh.dim_customers\") \\\n",
    "    .select(\"customer_id\", \"account_number\", \"customer_type\", \"territory_id\")\n",
    "\n",
    "df_dim_products = spark.table(\"data_mart_dlh.dim_products\") \\\n",
    "    .select(\"ProductID\", \"Name\", \"ProductNumber\", \"ListPrice\")\n",
    "\n",
    "df_dim_vendor = spark.table(\"data_mart_dlh.dim_vendor\") \\\n",
    "    .select(\"vendor_id\", \"vendor_name\")\n",
    "\n",
    "df_dim_date = spark.table(\"data_mart_dlh.dim_date\") \\\n",
    "    .select(\"date_key\", \"full_date\")\n",
    "\n",
    "# Standardize columns\n",
    "df_fact_orders = df_fact_orders \\\n",
    "    .withColumnRenamed(\"SalesOrderID\", \"sales_order_id\") \\\n",
    "    .withColumnRenamed(\"CustomerID\", \"customer_id\") \\\n",
    "    .withColumnRenamed(\"ProductID\", \"product_id\") \\\n",
    "    .withColumnRenamed(\"VendorID\", \"vendor_id\") \\\n",
    "    .withColumnRenamed(\"OrderDate\", \"order_date\") \\\n",
    "    .withColumnRenamed(\"OrderQty\", \"order_qty\") \\\n",
    "    .withColumnRenamed(\"LineTotal\", \"line_total\") \\\n",
    "    .withColumnRenamed(\"TotalDue\", \"total_due\")\n",
    "\n",
    "# Join with dim_date on order_date\n",
    "df_fact_orders = df_fact_orders \\\n",
    "    .withColumn(\"order_date\", to_date(\"order_date\")) \\\n",
    "    .join(df_dim_date.withColumn(\"full_date\", to_date(\"full_date\")), \n",
    "          col(\"order_date\") == col(\"full_date\"), \n",
    "          how=\"left\") \\\n",
    "    .drop(\"full_date\") \\\n",
    "    .withColumnRenamed(\"date_key\", \"order_date_key\")\n",
    "\n",
    "# Fill missing vendor_id and order_qty\n",
    "df_fact_orders = df_fact_orders.fillna({\n",
    "    \"vendor_id\": 0,\n",
    "    \"order_qty\": 0\n",
    "})\n",
    "\n",
    "# Drop duplicates by sales_order_id (if needed)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"sales_order_id\").orderBy(\"sales_order_id\")\n",
    "df_fact_orders = df_fact_orders.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                               .filter(\"row_num = 1\") \\\n",
    "                               .drop(\"row_num\")\n",
    "\n",
    "# Select and reorder final columns\n",
    "final_columns = [\n",
    "    \"sales_order_id\", \"customer_id\", \"order_date\", \"order_date_key\",\n",
    "    \"product_id\", \"vendor_id\", \"order_qty\", \"line_total\", \"total_due\"\n",
    "]\n",
    "df_fact_orders = df_fact_orders.select(final_columns)\n",
    "\n",
    "# Write to table\n",
    "df_fact_orders.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"data_mart_dlh.fact_orders\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM data_mart_dlh.fact_orders LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split and Export Fact Data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import os\n",
    "\n",
    "# 1. Create the destination directory for JSON mini-batches\n",
    "output_dir = \"/Users/vaneeshagupta/Desktop/capstone/stream_source/fact_orders\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 2. Add a row index to split evenly\n",
    "df_fact_orders = df_fact_orders.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# 3. Compute total row count and batch size\n",
    "row_count = df_fact_orders.count()\n",
    "batch_size = row_count // 3\n",
    "\n",
    "# 4. Create and write the 3 mini-batches\n",
    "df_batch_1 = df_fact_orders.filter(f\"row_id < {batch_size}\")\n",
    "df_batch_2 = df_fact_orders.filter(f\"row_id >= {batch_size} AND row_id < {2 * batch_size}\")\n",
    "df_batch_3 = df_fact_orders.filter(f\"row_id >= {2 * batch_size}\")\n",
    "\n",
    "# 5. Drop the helper column\n",
    "df_batch_1 = df_batch_1.drop(\"row_id\")\n",
    "df_batch_2 = df_batch_2.drop(\"row_id\")\n",
    "df_batch_3 = df_batch_3.drop(\"row_id\")\n",
    "\n",
    "# 6. Write each batch as JSON files into separate folders (simulate streaming intervals)\n",
    "df_batch_1.write.mode(\"overwrite\").json(f\"{output_dir}/batch1\")\n",
    "df_batch_2.write.mode(\"overwrite\").json(f\"{output_dir}/batch2\")\n",
    "df_batch_3.write.mode(\"overwrite\").json(f\"{output_dir}/batch3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use PySpark Structured Streaming to Process (Hot Path) Fact Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch1                         size=192 bytes   modified=1746824293.2371776\n",
      "batch2                         size=192 bytes   modified=1746824293.9083464\n",
      "batch3                         size=640 bytes   modified=1746824294.5005665\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "orders_stream_dir = \"stream_source/fact_orders\"\n",
    "files = os.listdir(orders_stream_dir)\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(orders_stream_dir, file)\n",
    "    size = os.path.getsize(file_path)\n",
    "    mtime = os.path.getmtime(file_path)\n",
    "    print(f\"{file:<30} size={size} bytes   modified={mtime}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, DateType\n",
    "\n",
    "# Define the schema manually\n",
    "fact_schema = StructType([\n",
    "    StructField(\"sales_order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"order_date_key\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"vendor_id\", DoubleType(), True),\n",
    "    StructField(\"order_qty\", IntegerType(), True),\n",
    "    StructField(\"line_total\", DoubleType(), True),\n",
    "    StructField(\"total_due\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Use recursiveFileLookup so it reads nested batch1, batch2, batch3 folders\n",
    "df_orders_bronze = (\n",
    "    spark.readStream\n",
    "    .option(\"schemaLocation\", orders_output_bronze)\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .schema(fact_schema)\n",
    "    .json(orders_stream_dir)\n",
    ")\n",
    "\n",
    "df_orders_bronze.isStreaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 29116b00-44d9-40bf-b1c6-8d103b82d9c1\n",
      "Query Name: orders_bronze\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "orders_checkpoint_bronze = os.path.join(orders_output_bronze, \"_checkpoint\")\n",
    "\n",
    "orders_bronze_query = (\n",
    "    df_orders_bronze\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .queryName(\"orders_bronze\")\n",
    "    .trigger(availableNow=True)  # complete one pass through all available files\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_bronze)\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .start(orders_output_bronze)\n",
    ")\n",
    "\n",
    "print(f\"Query ID: {orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {orders_bronze_query.status}\")\n",
    "\n",
    "# Wait for the stream to finish\n",
    "orders_bronze_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+--------------+----------+---------+---------+----------+----------+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sales_order_id|customer_id|order_date|order_date_key|product_id|vendor_id|order_qty|line_total|total_due |receipt_time           |source_file                                                                                                                        |\n",
      "+--------------+-----------+----------+--------------+----------+---------+---------+----------+----------+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|43661         |442        |2001-06-30|20010630      |773       |0.0      |2        |4079.988  |43561.4424|2025-05-09 15:16:00.046|file:///Users/vaneeshagupta/Desktop/test/stream_source/fact_orders/batch3/part-00004-0b1a0573-3442-41c3-b993-7286a0b166ba-c000.json|\n",
      "|43667         |646        |2001-06-30|20010630      |773       |0.0      |1        |2039.994  |8095.7863 |2025-05-09 15:15:59.329|file:///Users/vaneeshagupta/Desktop/test/stream_source/fact_orders/batch3/part-00002-0b1a0573-3442-41c3-b993-7286a0b166ba-c000.json|\n",
      "|43659         |676        |2001-06-30|20010630      |773       |0.0      |2        |4079.988  |27231.5495|2025-05-09 15:15:57.993|file:///Users/vaneeshagupta/Desktop/test/stream_source/fact_orders/batch1/part-00000-0539085c-0e38-4285-b615-adf7adb1cec5-c000.json|\n",
      "|43662         |227        |2001-06-30|20010630      |732       |0.0      |1        |356.898   |38331.9613|2025-05-09 15:15:59.05 |file:///Users/vaneeshagupta/Desktop/test/stream_source/fact_orders/batch3/part-00003-0b1a0573-3442-41c3-b993-7286a0b166ba-c000.json|\n",
      "|43673         |618        |2001-06-30|20010630      |712       |0.0      |1        |5.1865    |4944.3422 |2025-05-09 15:15:59.813|file:///Users/vaneeshagupta/Desktop/test/stream_source/fact_orders/batch3/part-00005-0b1a0573-3442-41c3-b993-7286a0b166ba-c000.json|\n",
      "+--------------+-----------+----------+--------------+----------+---------+---------+----------+----------+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bronze_path = \"/Users/vaneeshagupta/Desktop/capstone/bronze/fact_orders/part-*.parquet\"\n",
    "\n",
    "df_bronze = spark.read.parquet(bronze_path)\n",
    "df_bronze.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dimensions table\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType, IntegerType, LongType\n",
    "import os\n",
    "\n",
    "# Rename keys to match fact table for clean joins\n",
    "df_dim_customer = df_dim_customers.withColumnRenamed(\"CustomerID\", \"customer_id\")\n",
    "df_dim_vendor = df_dim_vendor.withColumnRenamed(\"VendorID\", \"vendor_id\")\n",
    "df_dim_product = df_dim_products.withColumnRenamed(\"ProductID\", \"product_id\")\n",
    "\n",
    "# Prepare role-playing dimensions\n",
    "df_dim_order_date = df_dim_date.select(\n",
    "    col(\"date_key\").alias(\"order_date_key\"),\n",
    "    col(\"full_date\").alias(\"order_full_date\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Paths\n",
    "orders_output_bronze = \"/Users/vaneeshagupta/Desktop/capstone/bronze/fact_orders\"\n",
    "orders_output_silver = \"/Users/vaneeshagupta/Desktop/capstone/silver/fact_orders\"\n",
    "orders_checkpoint_silver = os.path.join(orders_output_silver, \"_checkpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and transforming streaming data\n",
    "df_orders_silver = (\n",
    "    spark.readStream.format(\"parquet\").load(orders_output_bronze)\n",
    "    .withColumn(\"order_date\", col(\"order_date\").cast(DateType()))\n",
    "    .join(df_dim_customer, on=\"customer_id\", how=\"left\")\n",
    "    .join(df_dim_product, on=\"product_id\", how=\"left\")\n",
    "    .join(df_dim_vendor, on=\"vendor_id\", how=\"left\")\n",
    "    .join(\n",
    "        df_dim_order_date,\n",
    "        col(\"order_date\") == df_dim_order_date[\"order_full_date\"].cast(DateType()),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"sales_order_id\").cast(LongType()),\n",
    "        col(\"customer_id\").cast(LongType()),\n",
    "        col(\"product_id\").cast(LongType()),\n",
    "        col(\"vendor_id\").cast(LongType()),\n",
    "        df_dim_order_date[\"order_date_key\"].cast(LongType()),\n",
    "        col(\"order_qty\").cast(IntegerType()),\n",
    "        col(\"line_total\").cast(\"double\"),\n",
    "        col(\"total_due\").cast(\"double\"),\n",
    "        col(\"order_date\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_order_id: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- vendor_id: long (nullable = true)\n",
      " |-- order_date_key: long (nullable = true)\n",
      " |-- order_qty: integer (nullable = true)\n",
      " |-- line_total: double (nullable = true)\n",
      " |-- total_due: double (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 899b0c53-6a9d-4948-a034-3c0a34ef5944\n",
      "Query Name: orders_silver\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# Write to Silver\n",
    "orders_output_silver = \"/Users/vaneeshagupta/Desktop/test/silver/fact_orders\"\n",
    "orders_checkpoint_silver = \"/Users/vaneeshagupta/Desktop/test/silver/_checkpoint\"\n",
    "\n",
    "orders_silver_query = (\n",
    "    df_orders_silver.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_silver)\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .queryName(\"orders_silver\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start(orders_output_silver)\n",
    ")\n",
    "\n",
    "# Unit Test: Implement Query Monitoring\n",
    "print(f\"Query ID: {orders_silver_query.id}\")\n",
    "print(f\"Query Name: {orders_silver_query.name}\")\n",
    "print(f\"Query Status: {orders_silver_query.status}\")\n",
    "\n",
    "orders_silver_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold\n",
    "\n",
    "Total Revenue and Quantity by Customer and Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, month, year, sum, avg\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Join Silver fact_orders with dim_date\n",
    "df_orders_enriched = (\n",
    "    spark.readStream.format(\"parquet\").load(orders_output_silver)\n",
    "    .join(df_dim_date, col(\"order_date_key\") == df_dim_date.date_key, \"left\")\n",
    "    .withColumn(\"month\", month(col(\"full_date\").cast(DateType())))\n",
    "    .withColumn(\"year\", year(col(\"full_date\").cast(DateType())))\n",
    ")\n",
    "\n",
    "# Aggregate by customer\n",
    "df_orders_gold = (\n",
    "    df_orders_enriched\n",
    "    .groupBy(\"year\", \"month\", \"customer_id\")\n",
    "    .agg(\n",
    "        sum(\"line_total\").alias(\"total_revenue\"),\n",
    "        avg(\"order_qty\").alias(\"avg_order_qty\"),\n",
    "        sum(\"order_qty\").alias(\"total_items\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_gold_query = (\n",
    "    df_orders_gold.writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"fact_orders_by_customer_month\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+-------------+-------------+-----------+\n",
      "|year|month|customer_id|total_revenue|avg_order_qty|total_items|\n",
      "+----+-----+-----------+-------------+-------------+-----------+\n",
      "|2001|    6|        117|      874.794|          1.0|          1|\n",
      "|2001|    6|        203|      713.796|          2.0|          2|\n",
      "+----+-----+-----------+-------------+-------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "while not orders_gold_query.isActive:\n",
    "    time.sleep(1)\n",
    "\n",
    "spark.sql(\"SELECT * FROM fact_orders_by_customer_month ORDER BY year, month, customer_id\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final selection\n",
    "df_fact_orders_by_customer_month_final = df_orders_gold.selectExpr(\n",
    "    \"year as `Year`\",\n",
    "    \"month as `Month`\",\n",
    "    \"customer_id as `Customer ID`\",\n",
    "    \"total_revenue as `Total Revenue`\",\n",
    "    \"avg_order_qty as `Avg Order Qty`\",\n",
    "    \"total_items as `Total Items`\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_gold_query = (\n",
    "    df_fact_orders_by_customer_month_final.writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"fact_orders_by_customer_month_final_1\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "import time\n",
    "while not orders_gold_query.isActive:\n",
    "    time.sleep(1)\n",
    "\n",
    "df_static_final = spark.sql(\"SELECT * FROM fact_orders_by_customer_month_final\")\n",
    "\n",
    "df_static_final.write.saveAsTable(\n",
    "    f\"{dest_database}.fact_orders_by_customer_month_final\",\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Total Revenue</th>\n",
       "      <th>Avg Order Qty</th>\n",
       "      <th>Total Items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>510</td>\n",
       "      <td>419.4589</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>646</td>\n",
       "      <td>2039.9940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>117</td>\n",
       "      <td>874.7940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>442</td>\n",
       "      <td>4079.9880</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>227</td>\n",
       "      <td>356.8980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>203</td>\n",
       "      <td>713.7960</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>676</td>\n",
       "      <td>4079.9880</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>618</td>\n",
       "      <td>5.1865</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  Customer ID  Total Revenue  Avg Order Qty  Total Items\n",
       "0  2001      6          510       419.4589            1.0            1\n",
       "1  2001      6          646      2039.9940            1.0            1\n",
       "2  2001      6          117       874.7940            1.0            1\n",
       "3  2001      6          442      4079.9880            2.0            2\n",
       "4  2001      6          227       356.8980            1.0            1\n",
       "5  2001      6          203       713.7960            2.0            2\n",
       "6  2001      6          676      4079.9880            2.0            2\n",
       "7  2001      6          618         5.1865            1.0            1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_orders_by_customer_month_final\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
