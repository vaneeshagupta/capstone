{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "Dimensional data mart that represents a simple business involving customers, the products, and the vendors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Global Variables and Paths for Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"northwind_dw\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"Ashwaniis#1!\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"local\", # \"atlas\"\n",
    "    \"user_name\" : \"vaneeshagupta10\",\n",
    "    \"password\" : \"Fdztq26kWFlyBXiE\",\n",
    "    \"cluster_name\" : \"cluster0\",\n",
    "    \"cluster_subnet\" : \"koqso\",\n",
    "    \"db_name\" : \"northwind\",\n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = \"dbfs:/mnt/data\"  # DBFS path for your data\n",
    "data_dir = os.path.join(base_dir, 'source_data')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Streaming Data Source Directories\n",
    "# --------------------------------------------------------------------------------\n",
    "orders_stream_dir = os.path.join(stream_dir, 'orders')\n",
    "purchase_orders_stream_dir = os.path.join(stream_dir, 'purchase_orders')\n",
    "inventory_trans_stream_dir = os.path.join(stream_dir, 'inventory_transactions')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Databricks/SQL Warehouse Paths (Bronze, Silver, Gold Layers)\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"data_mart_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "orders_output_bronze = os.path.join(database_dir, 'fact_orders', 'bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'fact_orders', 'silver')\n",
    "orders_output_gold = os.path.join(database_dir, 'fact_orders', 'gold')\n",
    "\n",
    "purchase_orders_output_bronze = os.path.join(database_dir, 'fact_purchase_orders', 'bronze')\n",
    "purchase_orders_output_silver = os.path.join(database_dir, 'fact_purchase_orders', 'silver')\n",
    "purchase_orders_output_gold = os.path.join(database_dir, 'fact_purchase_orders', 'gold')\n",
    "\n",
    "inventory_trans_output_bronze = os.path.join(database_dir, 'fact_inventory_transactions', 'bronze')\n",
    "inventory_trans_output_silver = os.path.join(database_dir, 'fact_inventory_transactions', 'silver')\n",
    "inventory_trans_output_gold = os.path.join(database_dir, 'fact_inventory_transactions', 'gold')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Global Variables for MySQL, MongoDB, and File Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batches\")\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    return df_dropped\n",
    "\n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"driver\", args['conn_props']['driver']) \\\n",
    "        .option(\"user\", args['conn_props']['user']) \\\n",
    "        .option(\"password\", args['conn_props']['password']) \\\n",
    "        .option(\"query\", sql_query) \\\n",
    "        .load()\n",
    "    return dframe\n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "    return uri\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Northwind Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "    return client\n",
    "    \n",
    "def set_mongo_collections_with_pyspark(spark_session, data_directory: str, json_files: dict, **mongo_args):\n",
    "    db_name = mongo_args[\"db_name\"]\n",
    "    mongo_uri = get_mongo_uri(**mongo_args)\n",
    "\n",
    "    for collection_name, filename in json_files.items():\n",
    "        json_file_path = os.path.join(data_directory, filename)\n",
    "\n",
    "        df = spark_session.read \\\n",
    "            .option(\"multiline\", \"true\") \\\n",
    "            .json(json_file_path)\n",
    "\n",
    "        df.write \\\n",
    "            .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"uri\", mongo_uri) \\\n",
    "            .option(\"database\", db_name) \\\n",
    "            .option(\"collection\", collection_name) \\\n",
    "            .save()\n",
    "\n",
    "        print(f\"âœ” Loaded {filename} into MongoDB collection '{collection_name}'\")\n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "    dframe = dframe.drop('_id')\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    return dframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a New Metadata Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Lab 06 Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Lab 6.0');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Reference Data from MongoDB, MySQL, and CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MongoDB (Note: Customer Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Data from MongoDB\n",
    "# Get MongoDB client and fetch customer data\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "query = {}  # Select all elements (columns) and all documents (rows)\n",
    "collection_name = \"customers\"\n",
    "\n",
    "# Fetch MongoDB data into a DataFrame\n",
    "df_mongo_customers = get_mongo_dataframe(client, mongodb_args[\"db_name\"], collection_name, query)\n",
    "\n",
    "# Make Necessary Transformations to the DataFrame (Standardizing Column Names, etc.)\n",
    "df_mongo_customers.rename(columns={\"CustomerID\": \"customer_id\"}, inplace=True)\n",
    "\n",
    "# Save as the dim_customer table in the Data Warehouse (MySQL)\n",
    "# Standardize the column names for the data mart\n",
    "df_mongo_customers = df_mongo_customers[['customer_id', 'TerritoryID', 'AccountNumber', 'CustomerType']]\n",
    "\n",
    "# Insert the transformed customer data into the data warehouse's dim_customer table\n",
    "set_dataframe(df_mongo_customers, table_name=\"dim_customer\", pk_column=\"customer_id\", db_operation=\"insert\", **mysql_args)\n",
    "\n",
    "# Unit Test: Describe and Preview the Table\n",
    "# Check the 'dim_customer' table description and preview\n",
    "spark.sql(f\"DESCRIBE EXTENDED data_mart.dim_customer\").show()\n",
    "spark.sql(f\"SELECT * FROM data_mart.dim_customer LIMIT 2\").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MySQL (Note: Product Data and Date Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Data from MySQL\n",
    "# Fetch product data from MySQL\n",
    "sql_product = \"SELECT ProductID, Name, ProductNumber, ListPrice FROM product;\"\n",
    "df_product_mysql = get_sql_dataframe(sql_product, **mysql_args)\n",
    "\n",
    "# Make Necessary Transformations to the DataFrame (Standardizing Column Names, etc.)\n",
    "df_product_mysql.rename(columns={\"ProductID\": \"product_id\", \"Name\": \"product_name\", \"ProductNumber\": \"product_code\", \"ListPrice\": \"list_price\"}, inplace=True)\n",
    "\n",
    "# Save as the dim_product table in the Data Warehouse (MySQL)\n",
    "# Standardize the column names for the data mart\n",
    "df_product_mysql = df_product_mysql[['product_id', 'product_name', 'product_code', 'list_price']]\n",
    "\n",
    "# Insert the transformed product data into the data warehouse's dim_product table\n",
    "set_dataframe(df_product_mysql, table_name=\"dim_product\", pk_column=\"product_id\", db_operation=\"insert\", **mysql_args)\n",
    "\n",
    "# Unit Test: Describe and Preview the Table\n",
    "# Check the 'dim_product' table description and preview\n",
    "spark.sql(f\"DESCRIBE EXTENDED data_mart.dim_product\").show()\n",
    "spark.sql(f\"SELECT * FROM data_mart.dim_product LIMIT 2\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the Date Dimension Data from MySQL\n",
    "sql_dim_date = \"SELECT date_key, full_date FROM data_mart.dim_date;\"\n",
    "df_dim_date = get_sql_dataframe(sql_dim_date, **mysql_args)\n",
    "\n",
    "# Ensure the 'full_date' column is in datetime format for merging\n",
    "df_dim_date['full_date'] = pd.to_datetime(df_dim_date['full_date']).dt.date\n",
    "\n",
    "# Show the first few rows of the data to confirm the structure\n",
    "df_dim_date.head()\n",
    "\n",
    "# Convert to a PySpark DataFrame\n",
    "df_dim_date_spark = spark.createDataFrame(df_dim_date)\n",
    "\n",
    "# Check the schema of the date dimension to ensure correctness\n",
    "df_dim_date_spark.printSchema()\n",
    "\n",
    "# Save the date dimension as a table in your Data Warehouse (Data Lakehouse)\n",
    "df_dim_date_spark.write.saveAsTable(\"data_mart.dim_date\", mode=\"overwrite\")\n",
    "\n",
    "# Unit Test: Describe and Preview the Table\n",
    "# Check the structure of the dim_date table\n",
    "spark.sql(f\"DESCRIBE EXTENDED data_mart.dim_date\").show()\n",
    "\n",
    "# Preview the first few rows from the dim_date table to confirm data insertion\n",
    "spark.sql(f\"SELECT * FROM data_mart.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV File Using PySpark (Note: Vendor Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "vendor_csv_path = os.path.join(data_dir, 'vendor.csv')\n",
    "\n",
    "df_vendor = spark.read.format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .load(vendor_csv_path)\n",
    "\n",
    "# Show the first few rows to check the data\n",
    "df_vendor.show(5)\n",
    "\n",
    "# Rename columns to standardize them according to the data mart schema\n",
    "df_vendor = df_vendor.withColumnRenamed(\"VendorID\", \"vendor_id\") \\\n",
    "    .withColumnRenamed(\"Name\", \"vendor_name\") \\\n",
    "    .withColumnRenamed(\"AccountNumber\", \"vendor_account_number\") \\\n",
    "    .withColumnRenamed(\"CreditRating\", \"credit_rating\") \\\n",
    "    .withColumnRenamed(\"PreferredVendorStatus\", \"preferred_vendor_status\") \\\n",
    "    .withColumnRenamed(\"ActiveFlag\", \"active_flag\") \\\n",
    "    .withColumnRenamed(\"PurchasingWebServiceURL\", \"purchasing_web_service_url\") \\\n",
    "    .withColumnRenamed(\"ModifiedDate\", \"modified_date\")\n",
    "\n",
    "# Drop any columns that are not required for the dimension table\n",
    "df_vendor = df_vendor.select(\"vendor_id\", \"vendor_name\", \"vendor_account_number\", \"credit_rating\", \n",
    "                             \"preferred_vendor_status\", \"active_flag\", \"purchasing_web_service_url\", \"modified_date\")\n",
    "\n",
    "# Show the transformed DataFrame to check\n",
    "df_vendor.show(5)\n",
    "\n",
    "# Save as the dim_vendor table in the Data Warehouse\n",
    "set_dataframe(df_vendor, table_name=\"dim_vendor\", pk_column=\"vendor_id\", db_operation=\"insert\", **mysql_args)\n",
    "\n",
    "# Unit Test: Describe and Preview the Table\n",
    "# Check the structure of the dim_vendor table\n",
    "spark.sql(f\"DESCRIBE EXTENDED data_mart.dim_vendor\").show()\n",
    "\n",
    "# Preview the first few rows from the dim_vendor table to confirm data insertion\n",
    "spark.sql(f\"SELECT * FROM data_mart.dim_vendor LIMIT 2\").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Fact Table Data (Sales Orders)\n",
    "sql_sales_order_header = \"\"\"\n",
    "    SELECT SalesOrderID, CustomerID, OrderDate, TotalDue \n",
    "    FROM adventureworks.SalesOrderHeader;\n",
    "\"\"\"\n",
    "df_sales_order_header = get_sql_dataframe(sql_sales_order_header, **mysql_args)\n",
    "\n",
    "# Extracting Order Details to get the ProductID\n",
    "sql_sales_order_detail = \"\"\"\n",
    "    SELECT SalesOrderID, ProductID, OrderQty, LineTotal \n",
    "    FROM adventureworks.SalesOrderDetail;\n",
    "\"\"\"\n",
    "df_sales_order_detail = get_sql_dataframe(sql_sales_order_detail, **mysql_args)\n",
    "\n",
    "# Merging the sales order header and order details\n",
    "df_fact_orders = pd.merge(df_sales_order_header, df_sales_order_detail, on='SalesOrderID', how='left')\n",
    "\n",
    "# Renaming columns for consistency\n",
    "df_fact_orders.rename(columns={\n",
    "    \"SalesOrderID\": \"sales_order_id\",\n",
    "    \"CustomerID\": \"customer_id\",\n",
    "    \"ProductID\": \"product_id\",\n",
    "    \"OrderDate\": \"order_date\",\n",
    "    \"OrderQty\": \"order_qty\",\n",
    "    \"LineTotal\": \"line_total\",\n",
    "    \"TotalDue\": \"total_due\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge with the customer dimension to get more information\n",
    "df_fact_orders = pd.merge(df_fact_orders, df_dim_customer, on='customer_id', how='left')\n",
    "\n",
    "# Merge with the vendor dimension to get vendor details\n",
    "df_fact_orders = pd.merge(df_fact_orders, df_dim_vendor, on='vendor_id', how='left')\n",
    "\n",
    "# Merge with the product dimension to get product details\n",
    "df_fact_orders = pd.merge(df_fact_orders, df_dim_product, on='product_id', how='left')\n",
    "\n",
    "# Merge with the date dimension to get the date details\n",
    "df_fact_orders = pd.merge(df_fact_orders, df_dim_date, left_on='order_date', right_on='full_date', how='left')\n",
    "\n",
    "# Drop the 'full_date' column from the date dimension (since it's now included as 'order_date')\n",
    "df_fact_orders.drop(['full_date'], axis=1, inplace=True)\n",
    "\n",
    "# Handle missing data\n",
    "df_fact_orders['vendor_id'].fillna(0, inplace=True)  # Replace NaN with 0\n",
    "df_fact_orders['order_qty'].fillna(0, inplace=True)  # Replace NaN with 0\n",
    "\n",
    "# Remove duplicates based on the primary key column\n",
    "df_fact_orders = df_fact_orders.drop_duplicates(subset='sales_order_id', keep='first')\n",
    "\n",
    "# Ensure the column types are correct\n",
    "df_fact_orders['order_qty'] = df_fact_orders['order_qty'].astype(int)\n",
    "\n",
    "# Save the fact table to MySQL (or another DB) using the set_dataframe function\n",
    "table_name = \"fact_orders\"\n",
    "primary_key = \"sales_order_id\"\n",
    "db_operation = \"insert\"\n",
    "\n",
    "# Insert data into MySQL database\n",
    "set_dataframe(df_fact_orders, table_name, primary_key, db_operation, **mysql_args)\n",
    "\n",
    "# Unit Test: Preview the fact table in the database\n",
    "spark.sql(f\"SELECT * FROM data_mart.fact_orders LIMIT 2\").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use PySpark Structured Streaming to Process (Hot Path) Orders Fact Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the orders streaming data (streaming data for new sales orders)\n",
    "orders_stream_dir = os.path.join(stream_dir, 'orders')\n",
    "\n",
    "# Create a streaming DataFrame to read JSON data (representing the incoming orders)\n",
    "df_orders_bronze = (\n",
    "    spark.readStream\n",
    "    .option(\"schemaLocation\", orders_output_bronze)  # Location for schema inference\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Max files per trigger (mini-batch)\n",
    "    .option(\"multiLine\", \"true\")  # Handle multi-line JSON\n",
    "    .json(orders_stream_dir)\n",
    ")\n",
    "\n",
    "# Check if the DataFrame is streaming\n",
    "df_orders_bronze.isStreaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Streaming Data to a Parquet File (Bronze Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkpoint_bronze = os.path.join(orders_output_bronze, '_checkpoint')\n",
    "\n",
    "# Write the incoming orders data to the bronze layer (Parquet format)\n",
    "orders_bronze_query = (\n",
    "    df_orders_bronze\n",
    "    .withColumn(\"receipt_time\", current_timestamp())  # Add timestamp\n",
    "    .withColumn(\"source_file\", input_file_name())  # Add source file for traceability\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .queryName(\"orders_bronze\")\n",
    "    .trigger(availableNow=True)  # Process available data immediately\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_bronze)\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .start(orders_output_bronze)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the stream processes at least one batch\n",
    "wait_until_stream_is_ready(orders_bronze_query, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Silver Layer: Integrate \"Cold-Path\" Data & Make Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the streaming orders (fact table) with the reference data (dimensions)\n",
    "df_dim_customer = spark.table(f\"{dest_database}.dim_customer\")\n",
    "df_dim_product = spark.table(f\"{dest_database}.dim_product\")\n",
    "df_dim_vendor = spark.table(f\"{dest_database}.dim_vendor\")\n",
    "df_dim_date = spark.table(f\"{dest_database}.dim_date\")\n",
    "\n",
    "df_orders_silver = (\n",
    "    spark.readStream.format(\"parquet\").load(orders_output_bronze)\n",
    "    .join(df_dim_customer, \"customer_id\")  # Join with customer dimension\n",
    "    .join(df_dim_product, \"product_id\")  # Join with product dimension\n",
    "    .join(df_dim_vendor, \"vendor_id\")  # Join with vendor dimension\n",
    "    .join(df_dim_date, df_dim_date.date_key == col(\"order_date_key\"), \"left_outer\")  # Join with date dimension\n",
    "    .select(\n",
    "        col(\"sales_order_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"product_id\"),\n",
    "        col(\"vendor_id\"),\n",
    "        col(\"order_date\"),\n",
    "        col(\"order_qty\"),\n",
    "        col(\"line_total\"),\n",
    "        col(\"total_due\"),\n",
    "        col(\"date_key\").alias(\"order_date_key\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check if the DataFrame is streaming\n",
    "df_orders_silver.isStreaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Transformed Streaming Data to the Data Lakehouse (Silver Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkpoint_silver = os.path.join(orders_output_silver, '_checkpoint')\n",
    "\n",
    "# Write the enriched orders data (silver layer) to the Data Lakehouse\n",
    "orders_silver_query = (\n",
    "    df_orders_silver.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .queryName(\"orders_silver\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_silver)\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .start(orders_output_silver)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Define a Query to Create a Business Report (Gold Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_by_product_category_gold = (\n",
    "    spark.readStream.format(\"parquet\").load(orders_output_silver)\n",
    "    .join(df_dim_product, \"product_id\")  # Join with product dimension\n",
    "    .join(df_dim_date, df_dim_date.date_key == col(\"order_date_key\"))\n",
    "    .groupBy(\"month_of_year\", \"category\", \"month_name\")\n",
    "    .agg(count(\"product_id\").alias(\"product_count\"))\n",
    "    .orderBy(asc(\"month_of_year\"), desc(\"product_count\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the Aggregated Data to Memory in \"Complete\" Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_gold_query = (\n",
    "    df_orders_by_product_category_gold.writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .queryName(\"fact_orders_by_product_category\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the stream has processed the batches\n",
    "wait_until_stream_is_ready(orders_gold_query, 1)\n",
    "\n",
    "# Query the gold data from memory\n",
    "df_fact_orders_by_product_category = spark.sql(\"SELECT * FROM fact_orders_by_product_category\")\n",
    "df_fact_orders_by_product_category.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders_by_product_category_gold_final = df_fact_orders_by_product_category \\\n",
    "    .select(col(\"month_name\").alias(\"Month\"), col(\"category\").alias(\"Product Category\"), col(\"product_count\").alias(\"Product Count\")) \\\n",
    "    .orderBy(asc(\"month_of_year\"), desc(\"Product Count\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Final Results into a New Table and Display the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders_by_product_category_gold_final.write.saveAsTable(f\"{dest_database}.fact_orders_by_product_category\", mode=\"overwrite\")\n",
    "\n",
    "# Display the results of the gold layer\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_orders_by_product_category\").show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
